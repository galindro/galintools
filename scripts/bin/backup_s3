#!/usr/bin/python
import argparse, time, os, subprocess, re, sys, json
from datetime import datetime
from galintools import infra_common, monitoring, aws, windows_azure
from galintools.settings import *

zabbix_azure_value = 0

# Command line parsing
parser = argparse.ArgumentParser(description='Backup S3')

parser.add_argument('-r','--regions',
					nargs='+',
					default=settings['DEFAULT_REGION'].split(), 
					choices=settings['REGIONS'], 
					help='AWS Regions')

parser.add_argument('-a','--action', 
					default='all',
					choices=['backup','sync','all','zabbix_discovery'],
					help='Action to execute')

parser.add_argument('-c','--config', 
					required=True, 
					help='Config file')

args = parser.parse_args()

utils = infra_common.Utils()

config_parsed = utils.load_json_config(args.config)
if config_parsed == {}:
  exit(1)

try:
  logger = utils.create_new_logger(log_config=config_parsed['log'],
								   log_name=os.path.basename(__file__))
except Exception, e:
  logger = utils.create_new_logger(log_config=settings['log'],
								   log_name=os.path.basename(__file__))

if logger == 1:
  exit(1)

def set_zabbix_azure_value(val):
	global zabbix_azure_value
	zabbix_azure_value += val

def exec_thread(t):
	t.start()
	return t

def walklevel(some_dir, level=1):
	some_dir = some_dir.rstrip(os.path.sep)
	assert os.path.isdir(some_dir)
	num_sep = some_dir.count(os.path.sep)
	for root, dirs, files in os.walk(some_dir):
		yield root, dirs, files
		num_sep_this = root.count(os.path.sep)
		if num_sep + level <= num_sep_this:
			del dirs[:]

def s3_backup(config_parsed, bucket):
	origin = 's3://' + bucket
	dest = config_parsed['Backup']['destination'] + '/' + bucket + '/'
	log = config_parsed['Global']['aws_cmd_log_path'] + '/' + bucket + '.log'

	log_prefix = "bucket: " + bucket + "; destination: " + dest + ": "

	cmd = [config_parsed['Global']['aws_cmd'],
		   's3',
		   'sync',
		   origin,
		   dest]

	logger.info("%s Executing backup. Command: %s" % (log_prefix,cmd))

	f = open(log, 'a')
	p = subprocess.Popen(cmd, stdout=f, stderr=subprocess.PIPE)
	p.wait()
	f.close()

	zabbix_key = config_parsed['Backup']['zabbix_key'] + '[' + bucket + ']'

	if p.returncode != 0:
		logger.error("%s Error backing up s3 files. Details: %s" % (log_prefix, p.stderr.readlines()))
		utils.set_return_code(1)

		zabbix.zabbix_sender(key=zabbix_key,
							 value=1,
							 conf=config_parsed['Global']['zabbix_conf'] if 'zabbix_conf' in config_parsed['Global'] else None,
							 opts=config_parsed['Global']['zabbix_sender_opts'] if 'zabbix_sender_opts' in config_parsed['Global'] else None)

		return p.returncode
	else:
		logger.info("%s Backup completed successfully" % (log_prefix))

		zabbix.zabbix_sender(key=zabbix_key,
							 value=0,
							 conf=config_parsed['Global']['zabbix_conf'] if 'zabbix_conf' in config_parsed['Global'] else None,
							 opts=config_parsed['Global']['zabbix_sender_opts'] if 'zabbix_sender_opts' in config_parsed['Global'] else None)

		return 0

def sync_azure_s3(root_path,container):
	return_code = 0

	try:
		return_code = azure.azure_sync(root_path=root_path,
									   container=container)
	except Exception, e:
		logger.error("Error backing up s3 files. Details: %s" % (e))
		utils.set_return_code(1)
		return_code = 1

	set_zabbix_azure_value(return_code)
	return return_code

zabbix = monitoring.Zabbix(logger=logger,
						   server=config_parsed['Global']['zabbix_server'],
						   hostname=config_parsed['Global']['zabbix_host'])

for region in args.regions:

	aws_s3 = aws.S3(logger=logger, region=region)

	all_buckets = aws_s3.get_buckets()
	all_buckets_names = []

	for b in all_buckets:
		all_buckets_names.append(b.name)

	origin_regexp = re.compile(config_parsed['Backup']['origin']).search
	buckets = utils.filter_list(all_buckets_names,origin_regexp)

	if args.action == 'zabbix_discovery':
		zbx = {'data':[]}
		for b in buckets:
			zbx['data'].append({'{#S3_BUCKET}':b})
		print json.dumps(zbx)

	else:
		if args.action == 'backup' or args.action == 'all':

			parallel_process = int(config_parsed['Default']['parallel_process'] if 'parallel_process' not in config_parsed['Backup'] else config_parsed['Backup']['parallel_process'])

			if not os.path.exists(config_parsed['Backup']['destination']):
				try:
					os.makedirs(config_parsed['Backup']['destination'])
				except Exception, e:
					logger.error("Error creating directory %s. Details: " % (config_parsed['Backup']['destination'], e))
					utils.set_return_code(1)
					exit(utils.return_code)

			if not os.path.exists(os.path.dirname(config_parsed['Global']['aws_cmd_log_path'])):
				try:
					os.makedirs(os.path.dirname(config_parsed['Global']['aws_cmd_log_path']))
				except Exception, e:
					logger.error("Error creating directory %s. Details: " % (config_parsed['Global']['aws_cmd_log_path'], e))
					utils.set_return_code(1)
					exit(utils.return_code)

			for bucket in buckets:
				t = exec_thread(infra_common.NewThread(bucket, s3_backup, config_parsed, bucket))
				time.sleep(2)
				active_count = t.active_count() - 1

				while active_count >= parallel_process:
					time.sleep(5)
					active_count = t.active_count() - 1

			while active_count >= 2:
				time.sleep(5)
				active_count = t.active_count() - 1


		if args.action == 'sync' or args.action == 'all':
			if 'sync' in config_parsed['Backup']:
				for replication in config_parsed['Backup']['replication']:
					if replication == 'azure':
						azure = windows_azure.AzureBlobService(logger=logger,
															   account_name=config_parsed['Backup']['replication'][replication]['account_name'],
															   account_key=config_parsed['Backup']['replication'][replication]['account_key'])

						parallel_process = int(config_parsed['Default']['parallel_process'] if 'parallel_process' not in config_parsed['Backup']['replication'][replication] else config_parsed['Backup']['replication'][replication]['parallel_process'])

						for root, dirs, files in walklevel(some_dir=config_parsed['Backup']['destination'] if 'sync_from' not in config_parsed['Backup']['replication'][replication] else config_parsed['Backup']['replication'][replication]['sync_from'], 
														   level=1 if 'dir_depth_level' not in config_parsed['Backup']['replication'][replication] else int(config_parsed['Backup']['replication'][replication]['dir_depth_level'])):
							for dir in dirs:
								t = exec_thread(infra_common.NewThread(sync_azure_s3, 
																	   os.path.join(root,dir),
																	   config_parsed['Backup']['replication'][replication]['container']))
								time.sleep(2)
								active_count = t.active_count() - 1

								while active_count >= parallel_process:
									time.sleep(5)
									active_count = t.active_count() - 1

						while active_count >= 2:
							time.sleep(5)
							active_count = t.active_count() - 1

						zabbix.zabbix_sender(key=config_parsed['Backup']['replication'][replication]['zabbix_key'],
											 value=zabbix_azure_value,
											 conf=config_parsed['Global']['zabbix_conf'] if 'zabbix_conf' in config_parsed['Global'] else None,
											 opts=config_parsed['Global']['zabbix_sender_opts'] if 'zabbix_sender_opts' in config_parsed['Global'] else None)

exit(utils.return_code)
